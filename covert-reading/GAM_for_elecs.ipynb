{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "import torch.nn.functional as F\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import  StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "import torch.utils.data as Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "\n",
    "# 定义神经网络模型\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.temporal = nn.Conv1d(in_channels=256,out_channels=256,kernel_size=3)\n",
    "        self.temporal2 = nn.Conv1d(in_channels=256,out_channels=256,kernel_size=3)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,out_channels=3,kernel_size=5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv2d(3, 3, 5)\n",
    "        self.fc1 = nn.Linear(256*38, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 6)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.temporal(x)))\n",
    "        x = self.pool1(F.relu(self.temporal2(x)))\n",
    "        x = torch.unsqueeze(x,1)\n",
    "        #x = self.pool(F.relu(self.conv1(x)))\n",
    "        #x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "\n",
    "        x = x.view(-1,256*38)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class CBAMLayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16, spatial_kernel=7):\n",
    "        super(CBAMLayer, self).__init__()\n",
    "\n",
    "        # channel attention 压缩H,W为1\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # shared MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            # Conv2d比Linear方便操作\n",
    "            # nn.Linear(channel, channel // reduction, bias=False)\n",
    "            nn.Conv2d(channel, channel // reduction, 1, bias=False),\n",
    "            # inplace=True直接替换，节省内存\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Linear(channel // reduction, channel,bias=False)\n",
    "            nn.Conv2d(channel // reduction, channel, 1, bias=False)\n",
    "        )\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        max_out = self.mlp(self.max_pool(x))\n",
    "        avg_out = self.mlp(self.avg_pool(x))\n",
    "        channel_out = self.sigmoid(max_out + avg_out)\n",
    "        x = channel_out * x\n",
    "\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53, 256, 160)\n",
      "(57, 256, 160)\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as scio\n",
    "clean_path = \"D:\\\\BaiduSyncdisk\\\\code\"\n",
    "import scipy.io as scio\n",
    "import os\n",
    "import numpy as np\n",
    "forward = int(40)\n",
    "backward = int(120)\n",
    "#onset_time = {\"HS44\":0.368,\"HS45\":0.151,\"HS47\":0.156,\"HS48\":0.172,\"HS50\":0.357,\"HS54\":0.322}\n",
    "onset_time = {\"HS44\":0.368,\"HS45\":0.151,\"HS47\":0.156,\"HS48\":0.172,\"HS50\":0.357,\"HS54\":0.322}\n",
    "\n",
    "HS = 54\n",
    "file_name = \"HS\"+str(HS)+\"_Block_overt_covert.mat\"\n",
    "z3 = scio.loadmat(os.path.join(clean_path,file_name))\n",
    "z3 = z3[\"Alldata\"][0][0]\n",
    "\n",
    "onset = int(np.floor(100 * onset_time[\"HS\" + str(HS)])) + 150\n",
    "x1 = z3[\"ECoG_\"+\"overt_ba\"][:,:,onset-forward:onset+backward]\n",
    "x2 = z3[\"ECoG_\"+\"overt_da\"][:,:,onset-forward:onset+backward]\n",
    "x3 = z3[\"ECoG_\"+\"overt_ga\"][:,:,onset-forward:onset+backward]\n",
    "x4 = z3[\"ECoG_\"+\"overt_bu\"][:,:,onset-forward:onset+backward]\n",
    "x5 = z3[\"ECoG_\"+\"overt_du\"][:,:,onset-forward:onset+backward]\n",
    "x6 = z3[\"ECoG_\"+\"overt_gu\"][:,:,onset-forward:onset+backward]\n",
    "\n",
    "print(x1.shape)\n",
    "print(x2.shape)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 38])\n",
      "torch.Size([256, 1, 38])\n",
      "torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "net=Net()\n",
    "print(net(torch.Tensor(x2[0])).shape)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110,)\n",
      "[1,    20] loss: 0.082\n",
      "Accuracy of the network on the test images: 12 %\n",
      "[1,    40] loss: 0.018\n",
      "Accuracy of the network on the test images: 15 %\n",
      "[1,    60] loss: 0.022\n",
      "Accuracy of the network on the test images: 15 %\n",
      "[2,    20] loss: 0.018\n",
      "Accuracy of the network on the test images: 22 %\n",
      "[2,    40] loss: 0.018\n",
      "Accuracy of the network on the test images: 20 %\n",
      "[2,    60] loss: 0.018\n",
      "Accuracy of the network on the test images: 20 %\n",
      "True on the test images: 255 %\n",
      "Accuracy of the network on the test images: 8 %\n",
      "[1,    20] loss: 0.062\n",
      "Accuracy of the network on the test images: 15 %\n",
      "[1,    40] loss: 0.021\n",
      "Accuracy of the network on the test images: 20 %\n",
      "[1,    60] loss: 0.022\n",
      "Accuracy of the network on the test images: 18 %\n",
      "[2,    20] loss: 0.020\n",
      "Accuracy of the network on the test images: 16 %\n",
      "[2,    40] loss: 0.018\n",
      "Accuracy of the network on the test images: 14 %\n",
      "[2,    60] loss: 0.018\n",
      "Accuracy of the network on the test images: 13 %\n",
      "True on the test images: 250 %\n",
      "Accuracy of the network on the test images: 14 %\n",
      "[1,    20] loss: 0.039\n",
      "Accuracy of the network on the test images: 17 %\n",
      "[1,    40] loss: 0.019\n",
      "Accuracy of the network on the test images: 15 %\n",
      "[1,    60] loss: 0.020\n",
      "Accuracy of the network on the test images: 15 %\n",
      "[2,    20] loss: 0.018\n",
      "Accuracy of the network on the test images: 22 %\n",
      "[2,    40] loss: 0.018\n",
      "Accuracy of the network on the test images: 21 %\n",
      "[2,    60] loss: 0.018\n",
      "Accuracy of the network on the test images: 18 %\n",
      "True on the test images: 226 %\n",
      "Accuracy of the network on the test images: 11 %\n",
      "[1,    20] loss: 0.026\n",
      "Accuracy of the network on the test images: 8 %\n",
      "[1,    40] loss: 0.018\n",
      "Accuracy of the network on the test images: 10 %\n",
      "[1,    60] loss: 0.018\n",
      "Accuracy of the network on the test images: 10 %\n",
      "[2,    20] loss: 0.018\n",
      "Accuracy of the network on the test images: 15 %\n",
      "[2,    40] loss: 0.018\n",
      "Accuracy of the network on the test images: 12 %\n",
      "[2,    60] loss: 0.018\n",
      "Accuracy of the network on the test images: 15 %\n",
      "True on the test images: 229 %\n",
      "Accuracy of the network on the test images: 23 %\n",
      "[1,    20] loss: 0.056\n",
      "Accuracy of the network on the test images: 15 %\n",
      "[1,    40] loss: 0.020\n",
      "Accuracy of the network on the test images: 15 %\n",
      "[1,    60] loss: 0.018\n",
      "Accuracy of the network on the test images: 15 %\n",
      "[2,    20] loss: 0.026\n",
      "Accuracy of the network on the test images: 20 %\n",
      "[2,    40] loss: 0.018\n",
      "Accuracy of the network on the test images: 18 %\n",
      "[2,    60] loss: 0.018\n",
      "Accuracy of the network on the test images: 15 %\n",
      "True on the test images: 264 %\n",
      "Accuracy of the network on the test images: 20 %\n",
      "[1,    20] loss: 0.030\n",
      "Accuracy of the network on the test images: 13 %\n",
      "[1,    40] loss: 0.021\n",
      "Accuracy of the network on the test images: 20 %\n",
      "[1,    60] loss: 0.018\n",
      "Accuracy of the network on the test images: 16 %\n",
      "[2,    20] loss: 0.019\n",
      "Accuracy of the network on the test images: 23 %\n",
      "[2,    40] loss: 0.023\n",
      "Accuracy of the network on the test images: 25 %\n",
      "[2,    60] loss: 0.020\n",
      "Accuracy of the network on the test images: 22 %\n",
      "True on the test images: 229 %\n",
      "Accuracy of the network on the test images: 20 %\n",
      "[1,    20] loss: 0.029\n",
      "Accuracy of the network on the test images: 26 %\n",
      "[1,    40] loss: 0.023\n",
      "Accuracy of the network on the test images: 23 %\n",
      "[1,    60] loss: 0.018\n",
      "Accuracy of the network on the test images: 20 %\n",
      "[2,    20] loss: 0.018\n",
      "Accuracy of the network on the test images: 12 %\n",
      "[2,    40] loss: 0.018\n",
      "Accuracy of the network on the test images: 13 %\n",
      "[2,    60] loss: 0.019\n",
      "Accuracy of the network on the test images: 16 %\n",
      "True on the test images: 250 %\n",
      "Accuracy of the network on the test images: 8 %\n",
      "[1,    20] loss: 0.042\n",
      "Accuracy of the network on the test images: 17 %\n",
      "[1,    40] loss: 0.021\n",
      "Accuracy of the network on the test images: 15 %\n",
      "[1,    60] loss: 0.018\n",
      "Accuracy of the network on the test images: 15 %\n",
      "[2,    20] loss: 0.018\n",
      "Accuracy of the network on the test images: 15 %\n",
      "[2,    40] loss: 0.018\n",
      "Accuracy of the network on the test images: 14 %\n",
      "[2,    60] loss: 0.020\n",
      "Accuracy of the network on the test images: 14 %\n",
      "True on the test images: 288 %\n",
      "Accuracy of the network on the test images: 14 %\n",
      "[1,    20] loss: 0.044\n",
      "Accuracy of the network on the test images: 16 %\n",
      "[1,    40] loss: 0.021\n",
      "Accuracy of the network on the test images: 13 %\n",
      "[1,    60] loss: 0.018\n",
      "Accuracy of the network on the test images: 17 %\n",
      "[2,    20] loss: 0.018\n",
      "Accuracy of the network on the test images: 8 %\n",
      "[2,    40] loss: 0.019\n",
      "Accuracy of the network on the test images: 13 %\n",
      "[2,    60] loss: 0.018\n",
      "Accuracy of the network on the test images: 14 %\n",
      "True on the test images: 285 %\n",
      "Accuracy of the network on the test images: 14 %\n",
      "[1,    20] loss: 0.035\n",
      "Accuracy of the network on the test images: 17 %\n",
      "[1,    40] loss: 0.019\n",
      "Accuracy of the network on the test images: 16 %\n",
      "[1,    60] loss: 0.018\n",
      "Accuracy of the network on the test images: 14 %\n",
      "[2,    20] loss: 0.018\n",
      "Accuracy of the network on the test images: 15 %\n",
      "[2,    40] loss: 0.018\n",
      "Accuracy of the network on the test images: 16 %\n",
      "[2,    60] loss: 0.018\n",
      "Accuracy of the network on the test images: 16 %\n",
      "True on the test images: 247 %\n",
      "Accuracy of the network on the test images: 17 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 加载数据集\n",
    "net=Net()\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "x = np.concatenate((x1,x2,x3,x4,x5,x6),axis=0)\n",
    "y = np.ones([len(x1)+len(x2)])\n",
    "y[:len(x1)]=0\n",
    "print(y.shape)\n",
    "y = np.concatenate((0*np.ones([len(x1)]),np.ones([len(x2)]),2*np.ones([len(x3)]),3*np.ones([len(x4)]),4*np.ones([len(x5)]),5*np.ones([len(x6)])),axis=0)\n",
    "BATCH_SIZE = 4\n",
    "trainset = Data.TensorDataset(torch.Tensor(x) ,torch.LongTensor(y))\n",
    "loader = Data.DataLoader(\n",
    "    dataset=trainset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    "    )\n",
    "# 五折交叉验证\n",
    "kf = KFold(n_splits=10,shuffle=True)\n",
    "for train_index, test_index in kf.split(trainset):\n",
    "    train_subset = torch.utils.data.Subset(trainset, train_index)\n",
    "    test_subset = torch.utils.data.Subset(trainset, test_index)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(train_subset, batch_size=4, shuffle=True, num_workers=1)\n",
    "    testloader = torch.utils.data.DataLoader(test_subset, batch_size=4, shuffle=False, num_workers=1)\n",
    "\n",
    "    net = Net()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "    for epoch in range(2):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            if i % 20 == 19:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "                print('Accuracy of the network on the test images: %d %%' % (100 * correct / total))\n",
    "\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # 设置模型为评估模式\n",
    "    net.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    correct_label = 0\n",
    "    # 不计算梯度\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            #print(outputs.data)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            correct_label += torch.sum(labels)\n",
    "    print('True on the test images: %d %%' % (100 * correct_label / total))\n",
    "    print('Accuracy of the network on the test images: %d %%' % (100 * correct / total))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fish's z Transform\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
